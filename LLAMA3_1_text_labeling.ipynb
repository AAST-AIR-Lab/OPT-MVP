{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "oWmKiLax_j1z"
      ],
      "authorship_tag": "ABX9TyPMrX9BueTfm2Q3ayd3Bp3U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "729fead45e604777a9de1b6a67352c49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c582b6169d445d08e471c9ab9ea6754",
              "IPY_MODEL_80356bd58da4420a94f26d49de0042c9",
              "IPY_MODEL_e8a0f5d2558c4dadaf5ae3af52d4b30f"
            ],
            "layout": "IPY_MODEL_1f2491d60fb74d98bfae12221a691eb1"
          }
        },
        "1c582b6169d445d08e471c9ab9ea6754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aab1e0ecdbf4b2a8c4cd53cf6f42cb0",
            "placeholder": "​",
            "style": "IPY_MODEL_5191b597bb0448d683264c516cd2bb41",
            "value": "capybarahermes-2.5-mistral-7b.Q8_0.gguf: 100%"
          }
        },
        "80356bd58da4420a94f26d49de0042c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e4ac27cbb5440c58f222cf40e498bfc",
            "max": 7695875136,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39c4d03340ca439d9a8261faa8654e0f",
            "value": 7695875136
          }
        },
        "e8a0f5d2558c4dadaf5ae3af52d4b30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fd29568522b4f82a4f3f6469e01099c",
            "placeholder": "​",
            "style": "IPY_MODEL_3f455559cdf64d59a552e0b0f0c770fe",
            "value": " 7.70G/7.70G [02:50&lt;00:00, 81.6MB/s]"
          }
        },
        "1f2491d60fb74d98bfae12221a691eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aab1e0ecdbf4b2a8c4cd53cf6f42cb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5191b597bb0448d683264c516cd2bb41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e4ac27cbb5440c58f222cf40e498bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39c4d03340ca439d9a8261faa8654e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6fd29568522b4f82a4f3f6469e01099c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f455559cdf64d59a552e0b0f0c770fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UndeadZed/OPT-MVP/blob/main/LLAMA3_1_text_labeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing all essentials for the Project"
      ],
      "metadata": {
        "id": "0KVmMgHB_Ma4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# solving the build problem by installing the missing scikit-build-core\n",
        "!pip install scikit-build-core==0.9.0\n",
        "# For download the models\n",
        "!pip install huggingface_hub\n",
        "#installing langchain for inferance\n",
        "!pip -q install langchain\n",
        "# installing Llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=61\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.62 --force-reinstall --upgrade --no-cache-dir --verbose --no-build-isolation"
      ],
      "metadata": {
        "id": "mY7GzY3dQQBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292c8620-5bd0-4fb2-cb8e-343b0a4a8fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-build-core==0.9.0\n",
            "  Downloading scikit_build_core-0.9.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-build-core==0.9.0) (1.2.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-build-core==0.9.0) (24.1)\n",
            "Collecting pathspec>=0.10.1 (from scikit-build-core==0.9.0)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-build-core==0.9.0) (2.0.2)\n",
            "Downloading scikit_build_core-0.9.0-py3-none-any.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.4/151.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: pathspec, scikit-build-core\n",
            "Successfully installed pathspec-0.12.1 scikit-build-core-0.9.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing pip 24.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python==0.2.62\n",
            "  Downloading llama_cpp_python-0.2.62.tar.gz (37.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.5/37.5 MB\u001b[0m \u001b[31m173.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command Preparing metadata (pyproject.toml)\n",
            "  *** scikit-build-core 0.9.0 using CMake 3.30.4 (metadata_wheel)\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/fb/25/ba023652a39a2c127200e85aed975fc6119b421e2c348e5d0171e2046edb/numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for jinja2>=2.11.3 from https://files.pythonhosted.org/packages/31/80/3a54838c3fb461f6fec263ebf3a3a41771bd05190238de3486aae8540c36/jinja2-3.1.4-py3-none-any.whl.metadata\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python==0.2.62)\n",
            "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/2e/0d/5d91ef2b4f30afa87483a3a7c108c777d144b1c42d7113459296a8a2bfa0/MarkupSafe-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading MarkupSafe-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m250.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m347.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m220.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading MarkupSafe-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "  *** scikit-build-core 0.9.0 using CMake 3.30.4 (wheel)\n",
            "  *** Configuring CMake...\n",
            "  loading initial cache file /tmp/tmpm0l8k4_o/build/CMakeInit.txt\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:397 (message):\n",
            "    LLAMA_CUBLAS is deprecated and will be removed in the future.\n",
            "\n",
            "    Use LLAMA_CUDA instead\n",
            "\n",
            "\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n",
            "  -- CUDA found\n",
            "  -- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 61\n",
            "  -- CUDA host compiler is GNU 11.4.0\n",
            "\n",
            "  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  CMake Warning (dev) at CMakeLists.txt:26 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  CMake Warning (dev) at CMakeLists.txt:35 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Configuring done (4.2s)\n",
            "  -- Generating done (0.0s)\n",
            "  -- Build files have been written to: /tmp/tmpm0l8k4_o/build\n",
            "  *** Building project with Unix Makefiles...\n",
            "  Change Dir: '/tmp/tmpm0l8k4_o/build'\n",
            "\n",
            "  Run Build Command(s): /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E env VERBOSE=1 /usr/bin/gmake -f Makefile\n",
            "  /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -S/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 -B/tmp/tmpm0l8k4_o/build --check-build-system CMakeFiles/Makefile.cmake 0\n",
            "  /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_progress_start /tmp/tmpm0l8k4_o/build/CMakeFiles /tmp/tmpm0l8k4_o/build//CMakeFiles/progress.marks\n",
            "  /usr/bin/gmake  -f CMakeFiles/Makefile2 all\n",
            "  gmake[1]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml.dir/build.make vendor/llama.cpp/CMakeFiles/ggml.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  cd /tmp/tmpm0l8k4_o/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp /tmp/tmpm0l8k4_o/build /tmp/tmpm0l8k4_o/build/vendor/llama.cpp /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/CMakeFiles/ggml.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml.dir/build.make vendor/llama.cpp/CMakeFiles/ggml.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [  1%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF CMakeFiles/ggml.dir/ggml.c.o.d -o CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml.c\n",
            "  [  3%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-alloc.c\n",
            "  [  5%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF CMakeFiles/ggml.dir/ggml-backend.c.o.d -o CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-backend.c\n",
            "  [  7%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF CMakeFiles/ggml.dir/ggml-quants.c.o.d -o CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-quants.c\n",
            "  [  9%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/acc.cu -o CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\n",
            "  [ 11%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/alibi.cu -o CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\n",
            "  [ 13%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/arange.cu -o CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\n",
            "  [ 15%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/argsort.cu -o CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\n",
            "  [ 16%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/binbcast.cu -o CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\n",
            "  [ 18%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/clamp.cu -o CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\n",
            "  [ 20%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/concat.cu -o CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\n",
            "  [ 22%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/convert.cu -o CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\n",
            "  [ 24%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/cpy.cu -o CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\n",
            "  [ 26%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/diagmask.cu -o CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\n",
            "  [ 28%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/dmmv.cu -o CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\n",
            "  [ 30%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/getrows.cu -o CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\n",
            "  [ 32%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/im2col.cu -o CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\n",
            "  [ 33%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/mmq.cu -o CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\n",
            "  [ 35%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/mmvq.cu -o CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\n",
            "  [ 37%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/norm.cu -o CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\n",
            "  [ 39%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/pad.cu -o CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\n",
            "  [ 41%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/pool2d.cu -o CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\n",
            "  [ 43%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/quantize.cu -o CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\n",
            "  [ 45%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/rope.cu -o CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\n",
            "  [ 47%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/scale.cu -o CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\n",
            "  [ 49%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/softmax.cu -o CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\n",
            "  [ 50%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/sumrows.cu -o CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\n",
            "  [ 52%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/tsembd.cu -o CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\n",
            "  [ 54%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/unary.cu -o CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\n",
            "  [ 56%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda/upscale.cu -o CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\n",
            "  [ 58%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 --options-file CMakeFiles/ggml.dir/includes_CUDA.rsp -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" -Xcompiler=-fPIC -use_fast_math -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic -march=native\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/ggml-cuda.cu -o CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 58%] Built target ggml\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_static.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_static.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  cd /tmp/tmpm0l8k4_o/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp /tmp/tmpm0l8k4_o/build /tmp/tmpm0l8k4_o/build/vendor/llama.cpp /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/CMakeFiles/ggml_static.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_static.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_static.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 60%] Linking CUDA static library libggml_static.a\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -P CMakeFiles/ggml_static.dir/cmake_clean_target.cmake\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/ggml_static.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libggml_static.a CMakeFiles/ggml.dir/ggml.c.o \"CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml.dir/ggml-backend.c.o\" \"CMakeFiles/ggml.dir/ggml-quants.c.o\" \"CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda.cu.o\"\n",
            "  /usr/bin/ranlib libggml_static.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 60%] Built target ggml_static\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_shared.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_shared.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  cd /tmp/tmpm0l8k4_o/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp /tmp/tmpm0l8k4_o/build /tmp/tmpm0l8k4_o/build/vendor/llama.cpp /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/CMakeFiles/ggml_shared.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/ggml_shared.dir/build.make vendor/llama.cpp/CMakeFiles/ggml_shared.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 62%] Linking CUDA shared library libggml_shared.so\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/ggml_shared.dir/link.txt --verbose=1\n",
            "  /usr/bin/g++ -fPIC -shared -Wl,-soname,libggml_shared.so -o libggml_shared.so @CMakeFiles/ggml_shared.dir/objects1.rsp @CMakeFiles/ggml_shared.dir/linkLibs.rsp -L\"/usr/local/cuda/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda/targets/x86_64-linux/lib\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 62%] Built target ggml_shared\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/llama.dir/build.make vendor/llama.cpp/CMakeFiles/llama.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  cd /tmp/tmpm0l8k4_o/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp /tmp/tmpm0l8k4_o/build /tmp/tmpm0l8k4_o/build/vendor/llama.cpp /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/CMakeFiles/llama.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/CMakeFiles/llama.dir/build.make vendor/llama.cpp/CMakeFiles/llama.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 64%] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF CMakeFiles/llama.dir/llama.cpp.o.d -o CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/llama.cpp\n",
            "  [ 66%] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode.cpp.o -MF CMakeFiles/llama.dir/unicode.cpp.o.d -o CMakeFiles/llama.dir/unicode.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/unicode.cpp\n",
            "  [ 67%] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/unicode-data.cpp.o -MF CMakeFiles/llama.dir/unicode-data.cpp.o.d -o CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/unicode-data.cpp\n",
            "  [ 69%] Linking CXX shared library libllama.so\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llama.dir/link.txt --verbose=1\n",
            "  /usr/bin/c++ -fPIC -O3 -DNDEBUG -shared -Wl,-soname,libllama.so -o libllama.so CMakeFiles/llama.dir/llama.cpp.o CMakeFiles/llama.dir/unicode.cpp.o \"CMakeFiles/llama.dir/unicode-data.cpp.o\" CMakeFiles/ggml.dir/ggml.c.o \"CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"CMakeFiles/ggml.dir/ggml-backend.c.o\" \"CMakeFiles/ggml.dir/ggml-quants.c.o\" \"CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\" \"CMakeFiles/ggml.dir/ggml-cuda.cu.o\"   -L/usr/local/cuda/targets/x86_64-linux/lib  -Wl,-rpath,/usr/local/cuda-12.2/targets/x86_64-linux/lib: /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a -ldl /usr/lib/x86_64-linux-gnu/librt.a -lcudadevrt -lcudart_static -lrt -lpthread -ldl\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 69%] Built target llama\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/build_info.dir/build.make vendor/llama.cpp/common/CMakeFiles/build_info.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  cd /tmp/tmpm0l8k4_o/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common /tmp/tmpm0l8k4_o/build /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common/CMakeFiles/build_info.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/build_info.dir/build.make vendor/llama.cpp/common/CMakeFiles/build_info.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 71%] Building CXX object vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF CMakeFiles/build_info.dir/build-info.cpp.o.d -o CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/build-info.cpp\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 73%] Built target build_info\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/common.dir/build.make vendor/llama.cpp/common/CMakeFiles/common.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  cd /tmp/tmpm0l8k4_o/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common /tmp/tmpm0l8k4_o/build /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common/CMakeFiles/common.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/common/CMakeFiles/common.dir/build.make vendor/llama.cpp/common/CMakeFiles/common.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 75%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF CMakeFiles/common.dir/common.cpp.o.d -o CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/common.cpp\n",
            "  [ 77%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF CMakeFiles/common.dir/sampling.cpp.o.d -o CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/sampling.cpp\n",
            "  [ 79%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF CMakeFiles/common.dir/console.cpp.o.d -o CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/console.cpp\n",
            "  [ 81%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF CMakeFiles/common.dir/grammar-parser.cpp.o.d -o CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/grammar-parser.cpp\n",
            "  [ 83%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -MF CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d -o CMakeFiles/common.dir/json-schema-to-grammar.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/json-schema-to-grammar.cpp\n",
            "  [ 84%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF CMakeFiles/common.dir/train.cpp.o.d -o CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/train.cpp\n",
            "  [ 86%] Building CXX object vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common && /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_SCHED_MAX_COPIES=4 -DGGML_USE_CUDA -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/ngram-cache.cpp.o -MF CMakeFiles/common.dir/ngram-cache.cpp.o.d -o CMakeFiles/common.dir/ngram-cache.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/ngram-cache.cpp\n",
            "  [ 88%] Linking CXX static library libcommon.a\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -P CMakeFiles/common.dir/cmake_clean_target.cmake\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/common && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/common.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libcommon.a CMakeFiles/common.dir/common.cpp.o CMakeFiles/common.dir/sampling.cpp.o CMakeFiles/common.dir/console.cpp.o \"CMakeFiles/common.dir/grammar-parser.cpp.o\" \"CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\" CMakeFiles/common.dir/train.cpp.o \"CMakeFiles/common.dir/ngram-cache.cpp.o\" \"CMakeFiles/build_info.dir/build-info.cpp.o\"\n",
            "  /usr/bin/ranlib libcommon.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 88%] Built target common\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  cd /tmp/tmpm0l8k4_o/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava /tmp/tmpm0l8k4_o/build /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 90%] Building CXX object vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava && /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF CMakeFiles/llava.dir/llava.cpp.o.d -o CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/llava.cpp\n",
            "  [ 92%] Building CXX object vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava && /usr/bin/c++ -DGGML_USE_CUBLAS -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF CMakeFiles/llava.dir/clip.cpp.o.d -o CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/clip.cpp\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 92%] Built target llava\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  cd /tmp/tmpm0l8k4_o/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava /tmp/tmpm0l8k4_o/build /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_static.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 94%] Linking CXX static library libllava_static.a\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -P CMakeFiles/llava_static.dir/cmake_clean_target.cmake\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llava_static.dir/link.txt --verbose=1\n",
            "  /usr/bin/ar qc libllava_static.a CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o\n",
            "  /usr/bin/ranlib libllava_static.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 94%] Built target llava_static\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  cd /tmp/tmpm0l8k4_o/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava /tmp/tmpm0l8k4_o/build /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava_shared.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 96%] Linking CXX shared library libllava.so\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llava_shared.dir/link.txt --verbose=1\n",
            "  /usr/bin/c++ -fPIC -O3 -DNDEBUG -shared -Wl,-soname,libllava.so -o libllava.so CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o ../../CMakeFiles/ggml.dir/ggml.c.o \"../../CMakeFiles/ggml.dir/ggml-alloc.c.o\" \"../../CMakeFiles/ggml.dir/ggml-backend.c.o\" \"../../CMakeFiles/ggml.dir/ggml-quants.c.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/acc.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/alibi.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/arange.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/argsort.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/binbcast.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/clamp.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/concat.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/convert.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/cpy.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/diagmask.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/dmmv.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/getrows.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/im2col.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/mmq.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/mmvq.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/norm.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/pad.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/pool2d.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/quantize.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/rope.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/scale.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/softmax.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/sumrows.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/tsembd.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/unary.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda/upscale.cu.o\" \"../../CMakeFiles/ggml.dir/ggml-cuda.cu.o\"  -Wl,-rpath,/tmp/tmpm0l8k4_o/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib: ../../libllama.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so -ldl /usr/lib/x86_64-linux-gnu/librt.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 96%] Built target llava_shared\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/depend\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  cd /tmp/tmpm0l8k4_o/build && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_depends \"Unix Makefiles\" /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234 /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava /tmp/tmpm0l8k4_o/build /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/DependInfo.cmake \"--color=\"\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/bin/gmake  -f vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/build.make vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/build\n",
            "  gmake[2]: Entering directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [ 98%] Building CXX object vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava && /usr/bin/c++ -DGGML_USE_CUBLAS -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/common/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/vendor/llama.cpp/examples/llava/llava-cli.cpp\n",
            "  [100%] Linking CXX executable llava-cli\n",
            "  cd /tmp/tmpm0l8k4_o/build/vendor/llama.cpp/examples/llava && /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_link_script CMakeFiles/llava-cli.dir/link.txt --verbose=1\n",
            "  /usr/bin/c++ -O3 -DNDEBUG \"CMakeFiles/llava-cli.dir/llava-cli.cpp.o\" CMakeFiles/llava.dir/llava.cpp.o CMakeFiles/llava.dir/clip.cpp.o -o llava-cli  -Wl,-rpath,/tmp/tmpm0l8k4_o/build/vendor/llama.cpp:/usr/local/cuda-12.2/targets/x86_64-linux/lib: ../../common/libcommon.a ../../libllama.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcudart.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublas.so /usr/local/cuda-12.2/targets/x86_64-linux/lib/libculibos.a /usr/local/cuda-12.2/targets/x86_64-linux/lib/libcublasLt.so /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so -ldl /usr/lib/x86_64-linux-gnu/librt.a\n",
            "  gmake[2]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  [100%] Built target llava-cli\n",
            "  gmake[1]: Leaving directory '/tmp/tmpm0l8k4_o/build'\n",
            "  /usr/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E cmake_progress_start /tmp/tmpm0l8k4_o/build/CMakeFiles 0\n",
            "\n",
            "  *** Installing project into wheel...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/include/ggml.h\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/include/ggml-alloc.h\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/include/ggml-backend.h\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/include/ggml-cuda.h\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/lib/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpm0l8k4_o/wheel/platlib/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/include/llama.h\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/bin/convert.py\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpm0l8k4_o/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/llama_cpp/libllama.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/lib/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpm0l8k4_o/wheel/platlib/lib/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/bin/llava-cli\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpm0l8k4_o/wheel/platlib/bin/llava-cli\" to \"\"\n",
            "  -- Installing: /tmp/tmpm0l8k4_o/wheel/platlib/llama_cpp/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpm0l8k4_o/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/llama_cpp/libllava.so\n",
            "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-j3ey1uf1/llama-cpp-python_8d8840cede7542789da2228e9d0b9234/llama_cpp/libllava.so\" to \"\"\n",
            "  *** Making wheel...\n",
            "  *** Created llama_cpp_python-0.2.62-cp310-cp310-linux_x86_64.whl...\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.62-cp310-cp310-linux_x86_64.whl size=18239231 sha256=68a2a0f8d20d889504046acb1ee63ede9478e4ccd01e43bb9f90d2ba155a1f0f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dgwn1pq5/wheels/c0/81/de/d4cc8f152d89865379dbf28ca672358c667192ee55deaca7cb\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.12.2.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.26.4.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "  changing mode of /usr/local/bin/numpy-config to 755\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/MarkupSafe-2.1.5.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/markupsafe/\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2-3.1.4.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/jinja2/\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.6.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.2 which is incompatible.\n",
            "langchain 0.3.3 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.1.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.2 which is incompatible.\n",
            "pytensor 2.25.5 requires numpy<2,>=1.17.0, but you have numpy 2.1.2 which is incompatible.\n",
            "rmm-cu12 24.6.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.2 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.1 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.62 numpy-2.1.2 typing-extensions-4.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the model"
      ],
      "metadata": {
        "id": "45olOyEqhhu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# model_name_or_path = \"TheBloke/Tess-34B-v1.5b-GGUF\"\n",
        "# model_basename = \"tess-34b-v1.5b.Q3_K_M.gguf\" # the model is in gguf format\n",
        "\n",
        "model_name_or_path = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\"\n",
        "model_basename = \"capybarahermes-2.5-mistral-7b.Q8_0.gguf\"\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "sojbXFGphgFd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "729fead45e604777a9de1b6a67352c49",
            "1c582b6169d445d08e471c9ab9ea6754",
            "80356bd58da4420a94f26d49de0042c9",
            "e8a0f5d2558c4dadaf5ae3af52d4b30f",
            "1f2491d60fb74d98bfae12221a691eb1",
            "3aab1e0ecdbf4b2a8c4cd53cf6f42cb0",
            "5191b597bb0448d683264c516cd2bb41",
            "9e4ac27cbb5440c58f222cf40e498bfc",
            "39c4d03340ca439d9a8261faa8654e0f",
            "6fd29568522b4f82a4f3f6469e01099c",
            "3f455559cdf64d59a552e0b0f0c770fe"
          ]
        },
        "outputId": "547d5438-e7f6-4bf2-e895-0d42b53bc25e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "capybarahermes-2.5-mistral-7b.Q8_0.gguf:   0%|          | 0.00/7.70G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "729fead45e604777a9de1b6a67352c49"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making LLAMACPP-python LLM object"
      ],
      "metadata": {
        "id": "oWmKiLax_j1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "from llama_cpp import Llama\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=4096, # Context window\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYCz4WsQ_s1I",
        "outputId": "619f695d-570a-4ed6-9df5-1a31805a9be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 23 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--LLaMA-Pro-8B-Instruct-GGUF/snapshots/ebde3204ccf6ec8f75fa784d013dbde43923c847/llama-pro-8b-instruct.Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q6_K:  282 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 40\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 13B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 8.36 B\n",
            "llm_load_print_meta: model size       = 6.39 GiB (6.56 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
            "llm_load_tensors: offloading 40 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 41/41 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   102.54 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  6436.63 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  2560.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2560.00 MiB, K (f16): 1280.00 MiB, V (f16): 1280.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1286\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '40', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
            "Using gguf chat template: {% for message in messages %}\n",
            "{% if message['role'] == 'user' %}\n",
            "{{ '<|user|>\n",
            "' + message['content'] }}\n",
            "{% elif message['role'] == 'assistant' %}\n",
            "{{ '<|assistant|>\n",
            "'  + message['content'] + eos_token }}\n",
            "{% endif %}\n",
            "{% if loop.last and add_generation_prompt %}\n",
            "{{ '<|assistant|>' }}\n",
            "{% endif %}\n",
            "{% endfor %}\n",
            "Using chat eos_token: </s>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Maing infrences based in LLAMAcpp-python object"
      ],
      "metadata": {
        "id": "RtoXzLb8f_1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a linear regression in python\"\n",
        "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''"
      ],
      "metadata": {
        "id": "e04la5XXgPvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = lcpp_llm(\n",
        "    prompt=prompt_template,\n",
        "    max_tokens=256,\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1.2,\n",
        "    top_k=50,\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    echo=True # return the prompt\n",
        ")\n",
        "\n",
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWLV6g7UgToo",
        "outputId": "b4123f63-c351-4e14-a3d2-f1bc026244fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   10649.51 ms\n",
            "llama_print_timings:      sample time =     145.70 ms /   256 runs   (    0.57 ms per token,  1757.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10649.32 ms /    39 tokens (  273.06 ms per token,     3.66 tokens per second)\n",
            "llama_print_timings:        eval time =    9347.42 ms /   255 runs   (   36.66 ms per token,    27.28 tokens per second)\n",
            "llama_print_timings:       total time =   20961.18 ms /   294 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
            "\n",
            "USER: Write a linear regression in python\n",
            "\n",
            "ASSISTANT:\n",
            "Here's an example of how to write a simple linear regression model using Python with the scikit-learn library:\n",
            "\n",
            "```python\n",
            "from sklearn.linear_model import LinearRegression\n",
            "import numpy as np\n",
            "\n",
            "# Create some sample data for demonstration purposes\n",
            "X = np.array([1, 2, 3, 4]).reshape(-1, 1) # input features (independent variables)\n",
            "y = np.array([5, 7, 9, 11]) # output values (dependent variable)\n",
            "\n",
            "# Instantiate the linear regression model\n",
            "model = LinearRegression()\n",
            "\n",
            "# Fit the model to our data\n",
            "model.fit(X, y)\n",
            "```\n",
            "In this code:\n",
            "- We imported `LinearRegression` from scikit-learn's `linear_model` module and `numpy`. \n",
            "- Then we created a simple set of input features (independent variables), X, and an output variable (dependent variable), y.\n",
            "- The reshape(-1, 1) method is used to change the shape of our array from [4] -> [-1]. It's necessary because linear regression expects arrays with two dimensions -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clearing llamacpp cache"
      ],
      "metadata": {
        "id": "tsiD4cNdh8y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lcpp_llm.reset()\n",
        "lcpp_llm.set_cache(None)\n",
        "lcpp_llm = None\n",
        "del lcpp_llm"
      ],
      "metadata": {
        "id": "XppUtYEaiAqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making an LLM object with Langchain"
      ],
      "metadata": {
        "id": "ABbcX0ArhVLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain\n",
        "!pip -q install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKSOKY0wgZA3",
        "outputId": "afdd0a00-435b-4f8d-811b-11c6292f2915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# template = \"\"\"\n",
        "# {question}\n",
        "#  \"\"\"\n",
        "# prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "DpxDoBP1hruD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Verbose is required to pass to the callback manager\n",
        "\n",
        "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Loading model,\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    max_tokens=1024,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        "    n_ctx=4096, # Context window\n",
        "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
        "    temperature = 0.3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99foyd_ziOnF",
        "outputId": "2718f70e-0e11-4e4b-eec2-69502b5df5a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--CapybaraHermes-2.5-Mistral-7B-GGUF/snapshots/8bea614edd9a2d5d9985a6e6c1ecc166261cacb8/capybarahermes-2.5-mistral-7b.Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = argilla_capybarahermes-2.5-mistral-7b\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  226 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 261/32002 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32002\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = argilla_capybarahermes-2.5-mistral-7b\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   132.82 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  7205.84 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'argilla_capybarahermes-2.5-mistral-7b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
            "Guessed chat format: chatml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"\"\"\n",
        "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {question}\n",
        "\n",
        "ASSISTANT:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Jo4scKa0Zj18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "satisfy\n",
        "a\n",
        "shipping\n",
        "contract\n",
        ",\n",
        "a\n",
        "total\n",
        "of\n",
        "at\n",
        "least\n",
        "60,000\n",
        "square\n",
        "feet\n",
        "of\n",
        "flooring\n",
        "much\n",
        "be\n",
        "shipped\n",
        "each\n",
        "week\n",
        ".\n",
        "Due\n",
        "to\n",
        "a\n",
        "labor\n",
        "shortage\n",
        "issue\n",
        ",\n",
        "no\n",
        "more\n",
        "than\n",
        "50,000\n",
        "square\n",
        "feet\n",
        "of\n",
        "hardwood\n",
        "and\n",
        "30,000\n",
        "square\n",
        "feet\n",
        "of\n",
        "vinyl\n",
        "can\n",
        "be\n",
        "produced\n",
        "weekly\n",
        ".\n",
        "If\n",
        "a\n",
        "square\n",
        "foot\n",
        "of\n",
        "hardwood\n",
        "flooring\n",
        "yields\n",
        "a\n",
        "profit\n",
        "of\n",
        "$\n",
        "2.5\n",
        "and\n",
        "a\n",
        "square\n",
        "foot\n",
        "of\n",
        "vinyl\n",
        "planks\n",
        "produces\n",
        "a\n",
        "$\n",
        "3\n",
        "profit\n",
        ",\n",
        "how\n",
        "many\n",
        "of\n",
        "each\n",
        "type\n",
        "of\n",
        "flooring\n",
        "should\n",
        "be\n",
        "made\n",
        "weekly\n",
        "to\n",
        "maximize\n",
        "the\n",
        "company\n",
        "'s\n",
        "profit\n",
        "?\n",
        "\n",
        "John\n",
        "has\n",
        "a\n",
        "300\n",
        "acre\n",
        "berry\n",
        "farm\n",
        "on\n",
        "which\n",
        "to\n",
        "plant\n",
        "blueberries\n",
        "and\n",
        "raspberries\n",
        ".\n",
        "John\n",
        "has\n",
        "$\n",
        "10000\n",
        "to\n",
        "spend\n",
        "on\n",
        "watering\n",
        "and\n",
        "575\n",
        "days\n",
        "worth\n",
        "of\n",
        "labor\n",
        "available\n",
        ".\n",
        "For\n",
        "each\n",
        "acre\n",
        "of\n",
        "blueberries\n",
        ",\n",
        "6\n",
        "days\n",
        "worth\n",
        "of\n",
        "labor\n",
        "and\n",
        "$\n",
        "22\n",
        "in\n",
        "watering\n",
        "costs\n",
        "is\n",
        "required\n",
        ".\n",
        "For\n",
        "each\n",
        "acre\n",
        "of\n",
        "raspberries\n",
        ",\n",
        "3\n",
        "days\n",
        "worth\n",
        "of\n",
        "labor\n",
        "and\n",
        "$\n",
        "25\n",
        "in\n",
        "watering\n",
        "costs\n",
        "is\n",
        "required\n",
        ".\n",
        "The\n",
        "profit\n",
        "per\n",
        "acre\n",
        "of\n",
        "blueberries\n",
        "is\n",
        "$\n",
        "56\n",
        "and\n",
        "the\n",
        "profit\n",
        "per\n",
        "acre\n",
        "of\n",
        "raspberries\n",
        "is\n",
        "$\n",
        "75\n",
        ".\n",
        "Formulate\n",
        "an\n",
        "LP\n",
        "problem\n",
        "in\n",
        "order\n",
        "to\n",
        "maximize\n",
        "profit\n",
        ".\n",
        "\n",
        "An\n",
        "electronics\n",
        "store\n",
        "wants\n",
        "to\n",
        "optimize\n",
        "how\n",
        "many\n",
        "phones\n",
        "and\n",
        "laptops\n",
        "are\n",
        "enough\n",
        "to\n",
        "keep\n",
        "in\n",
        "inventory\n",
        ".\n",
        "A\n",
        "phone\n",
        "will\n",
        "earn\n",
        "the\n",
        "store\n",
        "$\n",
        "120\n",
        "in\n",
        "profits\n",
        ",\n",
        "and\n",
        "a\n",
        "laptop\n",
        "will\n",
        "earn\n",
        "$\n",
        "40\n",
        ".\n",
        "A\n",
        "phone\n",
        "requires\n",
        "1\n",
        "sq\n",
        "ft\n",
        "of\n",
        "floor\n",
        "space\n",
        ",\n",
        "whereas\n",
        "a\n",
        "laptop\n",
        "requires\n",
        "4\n",
        "sq\n",
        "ft\n",
        ".\n",
        "In\n",
        "total\n",
        ",\n",
        "400\n",
        "sq\n",
        "ft\n",
        "of\n",
        "floor\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iAw5BHj1b2JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "prompt = \"\"\" 'System: the following [unlabeled text (1)] is an example of an unlabled text followed by the same text after labeling [labeled text (1)],as you can see labeled text consists of 4 columns with the first being the unlabeled text while the other three consisting of symbols and labels write [labeled text (2)] based on the labels and format of the labeled text presented prior.'\n",
        "\n",
        "## unlabeled text (1):\n",
        "\n",
        "Cautious\n",
        "Asset\n",
        "Investment\n",
        "has\n",
        "a\n",
        "total\n",
        "of\n",
        "$\n",
        "150,000\n",
        "to\n",
        "manage\n",
        "and\n",
        "decides\n",
        "to\n",
        "invest\n",
        "it\n",
        "in\n",
        "money\n",
        "market\n",
        "fund\n",
        ",\n",
        "which\n",
        "yields\n",
        "a\n",
        "2\n",
        "%\n",
        "return\n",
        "as\n",
        "well\n",
        "as\n",
        "in\n",
        "foreign\n",
        "bonds\n",
        ",\n",
        "which\n",
        "gives\n",
        "and\n",
        "average\n",
        "rate\n",
        "of\n",
        "return\n",
        "of\n",
        "10.2\n",
        "%\n",
        ".\n",
        "Internal\n",
        "policies\n",
        "require\n",
        "PAI\n",
        "to\n",
        "diversify\n",
        "the\n",
        "asset\n",
        "allocation\n",
        "so\n",
        "that\n",
        "the\n",
        "minimum\n",
        "investment\n",
        "in\n",
        "money\n",
        "market\n",
        "fund\n",
        "is\n",
        "40\n",
        "%\n",
        "of\n",
        "the\n",
        "total\n",
        "investment\n",
        ".\n",
        "Due\n",
        "to\n",
        "the\n",
        "risk\n",
        "of\n",
        "default\n",
        "of\n",
        "foreign\n",
        "countries\n",
        ",\n",
        "no\n",
        "more\n",
        "than\n",
        "40\n",
        "%\n",
        "of\n",
        "the\n",
        "total\n",
        "investment\n",
        "should\n",
        "be\n",
        "allocated\n",
        "to\n",
        "foreign\n",
        "bonds\n",
        ".\n",
        "How\n",
        "much\n",
        "should\n",
        "the\n",
        "Cautious\n",
        "Asset\n",
        "Investment\n",
        "allocate\n",
        "in\n",
        "each\n",
        "asset\n",
        "so\n",
        "as\n",
        "to\n",
        "maximize\n",
        "its\n",
        "average\n",
        "return\n",
        "?\n",
        "\n",
        "## labeled text (1):\n",
        "Cautious\t_\t_\tO\n",
        "Asset\t_\t_\tO\n",
        "Investment\t_\t_\tO\n",
        "has\t_\t_\tO\n",
        "a\t_\t_\tO\n",
        "total\t_\t_\tB-CONST_DIR\n",
        "of\t_\t_\tO\n",
        "$\t_\t_\tO\n",
        "150,000\t_\t_\tB-LIMIT\n",
        "to\t_\t_\tO\n",
        "manage\t_\t_\tO\n",
        "and\t_\t_\tO\n",
        "decides\t_\t_\tO\n",
        "to\t_\t_\tO\n",
        "invest\t_\t_\tO\n",
        "it\t_\t_\tO\n",
        "in\t_\t_\tO\n",
        "money\t_\t_\tB-VAR\n",
        "market\t_\t_\tI-VAR\n",
        "fund\t_\t_\tI-VAR\n",
        ",\t_\t_\tO\n",
        "which\t_\t_\tO\n",
        "yields\t_\t_\tO\n",
        "a\t_\t_\tO\n",
        "2\t_\t_\tB-PARAM\n",
        "%\t_\t_\tI-PARAM\n",
        "return\t_\t_\tB-OBJ_NAME\n",
        "as\t_\t_\tO\n",
        "well\t_\t_\tO\n",
        "as\t_\t_\tO\n",
        "in\t_\t_\tO\n",
        "foreign\t_\t_\tB-VAR\n",
        "bonds\t_\t_\tI-VAR\n",
        ",\t_\t_\tO\n",
        "which\t_\t_\tO\n",
        "gives\t_\t_\tO\n",
        "and\t_\t_\tO\n",
        "average\t_\t_\tO\n",
        "rate\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "return\t_\t_\tB-OBJ_NAME\n",
        "of\t_\t_\tO\n",
        "10.2\t_\t_\tB-PARAM\n",
        "%\t_\t_\tO\n",
        ".\t_\t_\tO\n",
        "Internal\t_\t_\tO\n",
        "policies\t_\t_\tO\n",
        "require\t_\t_\tO\n",
        "PAI\t_\t_\tO\n",
        "to\t_\t_\tO\n",
        "diversify\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "asset\t_\t_\tO\n",
        "allocation\t_\t_\tO\n",
        "so\t_\t_\tO\n",
        "that\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "minimum\t_\t_\tB-CONST_DIR\n",
        "investment\t_\t_\tO\n",
        "in\t_\t_\tO\n",
        "money\t_\t_\tB-VAR\n",
        "market\t_\t_\tI-VAR\n",
        "fund\t_\t_\tI-VAR\n",
        "is\t_\t_\tO\n",
        "40\t_\t_\tB-LIMIT\n",
        "%\t_\t_\tI-LIMIT\n",
        "of\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "total\t_\t_\tO\n",
        "investment\t_\t_\tO\n",
        ".\t_\t_\tO\n",
        "Due\t_\t_\tO\n",
        "to\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "risk\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "default\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "foreign\t_\t_\tO\n",
        "countries\t_\t_\tO\n",
        ",\t_\t_\tO\n",
        "no\t_\t_\tB-CONST_DIR\n",
        "more\t_\t_\tI-CONST_DIR\n",
        "than\t_\t_\tI-CONST_DIR\n",
        "40\t_\t_\tB-LIMIT\n",
        "%\t_\t_\tI-LIMIT\n",
        "of\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "total\t_\t_\tO\n",
        "investment\t_\t_\tO\n",
        "should\t_\t_\tO\n",
        "be\t_\t_\tO\n",
        "allocated\t_\t_\tO\n",
        "to\t_\t_\tO\n",
        "foreign\t_\t_\tB-VAR\n",
        "bonds\t_\t_\tI-VAR\n",
        ".\t_\t_\tO\n",
        "How\t_\t_\tO\n",
        "much\t_\t_\tO\n",
        "should\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "Cautious\t_\t_\tO\n",
        "Asset\t_\t_\tO\n",
        "Investment\t_\t_\tO\n",
        "allocate\t_\t_\tO\n",
        "in\t_\t_\tO\n",
        "each\t_\t_\tO\n",
        "asset\t_\t_\tO\n",
        "so\t_\t_\tO\n",
        "as\t_\t_\tO\n",
        "to\t_\t_\tO\n",
        "maximize\t_\t_\tB-OBJ_DIR\n",
        "its\t_\t_\tO\n",
        "average\t_\t_\tB-OBJ_NAME\n",
        "return\t_\t_\tI-OBJ_NAME\n",
        "?\t_\t_\tO\n",
        "\n",
        "A\t_\t_\tO\n",
        "flooring\t_\t_\tO\n",
        "company\t_\t_\tO\n",
        "produces\t_\t_\tO\n",
        "engineered\t_\t_\tO\n",
        "hardwood\t_\t_\tB-VAR\n",
        "and\t_\t_\tO\n",
        "vinyl\t_\t_\tB-VAR\n",
        "planks\t_\t_\tI-VAR\n",
        ".\t_\t_\tO\n",
        "Their\t_\t_\tO\n",
        "sales\t_\t_\tO\n",
        "forecasts\t_\t_\tO\n",
        "show\t_\t_\tO\n",
        "an\t_\t_\tO\n",
        "expected\t_\t_\tO\n",
        "demand\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "at\t_\t_\tB-CONST_DIR\n",
        "least\t_\t_\tI-CONST_DIR\n",
        "20,000\t_\t_\tB-LIMIT\n",
        "square\t_\t_\tO\n",
        "foot\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "hardwood\t_\t_\tB-VAR\n",
        "and\t_\t_\tO\n",
        "10,000\t_\t_\tB-LIMIT\n",
        "square\t_\t_\tO\n",
        "feet\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "vinyl\t_\t_\tB-VAR\n",
        "planks\t_\t_\tI-VAR\n",
        "each\t_\t_\tO\n",
        "week\t_\t_\tO\n",
        ".\t_\t_\tO\n",
        "To\t_\t_\tO\n",
        "\n",
        "## Unlabeled text (2):\n",
        "satisfy\n",
        "a\n",
        "shipping\n",
        "contract\n",
        ",\n",
        "a\n",
        "total\n",
        "of\n",
        "at\n",
        "least\n",
        "60,000\n",
        "square\n",
        "feet\n",
        "of\n",
        "flooring\n",
        "much\n",
        "be\n",
        "shipped\n",
        "each\n",
        "week\n",
        ".\n",
        "Due\n",
        "to\n",
        "a\n",
        "labor\n",
        "shortage\n",
        "issue\n",
        ",\n",
        "no\n",
        "more\n",
        "than\n",
        "50,000\n",
        "square\n",
        "feet\n",
        "of\n",
        "hardwood\n",
        "and\n",
        "30,000\n",
        "square\n",
        "feet\n",
        "of\n",
        "vinyl\n",
        "can\n",
        "be\n",
        "produced\n",
        "weekly\n",
        ".\n",
        "If\n",
        "a\n",
        "square\n",
        "foot\n",
        "of\n",
        "hardwood\n",
        "flooring\n",
        "yields\n",
        "a\n",
        "profit\n",
        "of\n",
        "$\n",
        "2.5\n",
        "and\n",
        "a\n",
        "square\n",
        "foot\n",
        "of\n",
        "vinyl\n",
        "planks\n",
        "produces\n",
        "a\n",
        "$\n",
        "3\n",
        "profit\n",
        ",\n",
        "how\n",
        "many\n",
        "of\n",
        "each\n",
        "type\n",
        "of\n",
        "flooring\n",
        "should\n",
        "be\n",
        "made\n",
        "weekly\n",
        "to\n",
        "maximize\n",
        "the\n",
        "company\n",
        "'s\n",
        "profit\n",
        "?\n",
        "\n",
        "John\n",
        "has\n",
        "a\n",
        "300\n",
        "acre\n",
        "berry\n",
        "farm\n",
        "on\n",
        "which\n",
        "to\n",
        "plant\n",
        "blueberries\n",
        "and\n",
        "raspberries\n",
        ".\n",
        "John\n",
        "has\n",
        "$\n",
        "10000\n",
        "to\n",
        "spend\n",
        "on\n",
        "watering\n",
        "and\n",
        "575\n",
        "days\n",
        "worth\n",
        "of\n",
        "labor\n",
        "available\n",
        ".\n",
        "For\n",
        "each\n",
        "acre\n",
        "of\n",
        "blueberries\n",
        ",\n",
        "6\n",
        "days\n",
        "worth\n",
        "of\n",
        "labor\n",
        "and\n",
        "$\n",
        "22\n",
        "in\n",
        "watering\n",
        "costs\n",
        "is\n",
        "required\n",
        ".\n",
        "For\n",
        "each\n",
        "acre\n",
        "of\n",
        "raspberries\n",
        ",\n",
        "3\n",
        "days\n",
        "worth\n",
        "of\n",
        "labor\n",
        "and\n",
        "$\n",
        "25\n",
        "in\n",
        "watering\n",
        "costs\n",
        "is\n",
        "required\n",
        ".\n",
        "The\n",
        "profit\n",
        "per\n",
        "acre\n",
        "of\n",
        "blueberries\n",
        "is\n",
        "$\n",
        "56\n",
        "and\n",
        "the\n",
        "profit\n",
        "per\n",
        "acre\n",
        "of\n",
        "raspberries\n",
        "is\n",
        "$\n",
        "75\n",
        ".\n",
        "Formulate\n",
        "an\n",
        "LP\n",
        "problem\n",
        "in\n",
        "order\n",
        "to\n",
        "maximize\n",
        "profit\n",
        ".\n",
        "\n",
        "An\n",
        "electronics\n",
        "store\n",
        "wants\n",
        "to\n",
        "optimize\n",
        "how\n",
        "many\n",
        "phones\n",
        "and\n",
        "laptops\n",
        "are\n",
        "enough\n",
        "to\n",
        "keep\n",
        "in\n",
        "inventory\n",
        ".\n",
        "A\n",
        "phone\n",
        "will\n",
        "earn\n",
        "the\n",
        "store\n",
        "$\n",
        "120\n",
        "in\n",
        "profits\n",
        ",\n",
        "and\n",
        "a\n",
        "laptop\n",
        "will\n",
        "earn\n",
        "$\n",
        "40\n",
        ".\n",
        "A\n",
        "phone\n",
        "requires\n",
        "11\n",
        "sq\n",
        "ft\n",
        "of\n",
        "floor\n",
        "space\n",
        ",\n",
        "whereas\n",
        "a\n",
        "laptop\n",
        "requires\n",
        "4\n",
        "sq\n",
        "ft\n",
        ".\n",
        "In\n",
        "total\n",
        ",\n",
        "400\n",
        "sq\n",
        "ft\n",
        "of\n",
        "floor\n",
        "## labeled text (2):\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "ih5dNhIyjaDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = f\"\"\"\n",
        "'System: the following [unlabeled text (1)] is an example of an unlabled text followed by the same text after labeling [labeled text (1)],as you can see labeled text consists of 4 columns with the first being the unlabeled text while the other three consisting of symbols and labels write [labeled text (2)] Label words as O for general words, B/I-CONST_DIR for constraint descriptions, B/I-LIMIT for numerical limits, B/I-VAR for variables, B/I-PARAM for measurable parameters, B/I-OBJ_NAME for objectives, and B-OBJ_DIR for actions toward the objective:\n",
        "\n",
        "\n",
        "## unlabeled text (1):\n",
        "\n",
        "Cautious\n",
        "Asset\n",
        "Investment\n",
        "has\n",
        "a\n",
        "total\n",
        "of\n",
        "$\n",
        "150,000\n",
        "to\n",
        "manage\n",
        "and\n",
        "decides\n",
        "to\n",
        "invest\n",
        "it\n",
        "in\n",
        "money\n",
        "market\n",
        "fund\n",
        ",\n",
        "which\n",
        "yields\n",
        "a\n",
        "2\n",
        "%\n",
        "return\n",
        "as\n",
        "well\n",
        "as\n",
        "in\n",
        "foreign\n",
        "bonds\n",
        ",\n",
        "which\n",
        "gives\n",
        "and\n",
        "average\n",
        "rate\n",
        "of\n",
        "return\n",
        "of\n",
        "10.2\n",
        "%\n",
        ".\n",
        "Internal\n",
        "policies\n",
        "require\n",
        "PAI\n",
        "to\n",
        "diversify\n",
        "the\n",
        "asset\n",
        "allocation\n",
        "so\n",
        "that\n",
        "the\n",
        "minimum\n",
        "investment\n",
        "in\n",
        "money\n",
        "market\n",
        "fund\n",
        "is\n",
        "40\n",
        "%\n",
        "of\n",
        "the\n",
        "total\n",
        "investment\n",
        ".\n",
        "Due\n",
        "to\n",
        "the\n",
        "risk\n",
        "of\n",
        "default\n",
        "of\n",
        "foreign\n",
        "countries\n",
        ",\n",
        "no\n",
        "more\n",
        "than\n",
        "40\n",
        "%\n",
        "of\n",
        "the\n",
        "total\n",
        "investment\n",
        "should\n",
        "be\n",
        "allocated\n",
        "to\n",
        "foreign\n",
        "bonds\n",
        ".\n",
        "How\n",
        "much\n",
        "should\n",
        "the\n",
        "Cautious\n",
        "Asset\n",
        "Investment\n",
        "allocate\n",
        "in\n",
        "each\n",
        "asset\n",
        "so\n",
        "as\n",
        "to\n",
        "maximize\n",
        "its\n",
        "average\n",
        "return\n",
        "?\n",
        "\n",
        "## labeled text (1):\n",
        "Cautious\t_\t_\tO\n",
        "Asset\t_\t_\tO\n",
        "Investment\t_\t_\tO\n",
        "has\t_\t_\tO\n",
        "a\t_\t_\tO\n",
        "total\t_\t_\tB-CONST_DIR\n",
        "of\t_\t_\tO\n",
        "$\t_\t_\tO\n",
        "150,000\t_\t_\tB-LIMIT\n",
        "to\t_\t_\tO\n",
        "manage\t_\t_\tO\n",
        "and\t_\t_\tO\n",
        "decides\t_\t_\tO\n",
        "to\t_\t_\tO\n",
        "invest\t_\t_\tO\n",
        "it\t_\t_\tO\n",
        "in\t_\t_\tO\n",
        "money\t_\t_\tB-VAR\n",
        "market\t_\t_\tI-VAR\n",
        "fund\t_\t_\tI-VAR\n",
        ",\t_\t_\tO\n",
        "which\t_\t_\tO\n",
        "yields\t_\t_\tO\n",
        "a\t_\t_\tO\n",
        "2\t_\t_\tB-PARAM\n",
        "%\t_\t_\tI-PARAM\n",
        "return\t_\t_\tB-OBJ_NAME\n",
        "as\t_\t_\tO\n",
        "well\t_\t_\tO\n",
        "as\t_\t_\tO\n",
        "in\t_\t_\tO\n",
        "foreign\t_\t_\tB-VAR\n",
        "bonds\t_\t_\tI-VAR\n",
        ",\t_\t_\tO\n",
        "which\t_\t_\tO\n",
        "gives\t_\t_\tO\n",
        "and\t_\t_\tO\n",
        "average\t_\t_\tO\n",
        "rate\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "return\t_\t_\tB-OBJ_NAME\n",
        "of\t_\t_\tO\n",
        "10.2\t_\t_\tB-PARAM\n",
        "%\t_\t_\tO\n",
        ".\t_\t_\tO\n",
        "Internal\t_\t_\tO\n",
        "policies\t_\t_\tO\n",
        "require\t_\t_\tO\n",
        "PAI\t_\t_\tO\n",
        "to\t_\t_\tO\n",
        "diversify\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "asset\t_\t_\tO\n",
        "allocation\t_\t_\tO\n",
        "so\t_\t_\tO\n",
        "that\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "minimum\t_\t_\tB-CONST_DIR\n",
        "investment\t_\t_\tO\n",
        "in\t_\t_\tO\n",
        "money\t_\t_\tB-VAR\n",
        "market\t_\t_\tI-VAR\n",
        "fund\t_\t_\tI-VAR\n",
        "is\t_\t_\tO\n",
        "40\t_\t_\tB-LIMIT\n",
        "%\t_\t_\tI-LIMIT\n",
        "of\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "total\t_\t_\tO\n",
        "investment\t_\t_\tO\n",
        ".\t_\t_\tO\n",
        "Due\t_\t_\tO\n",
        "to\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "risk\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "default\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "foreign\t_\t_\tO\n",
        "countries\t_\t_\tO\n",
        ",\t_\t_\tO\n",
        "no\t_\t_\tB-CONST_DIR\n",
        "more\t_\t_\tI-CONST_DIR\n",
        "than\t_\t_\tI-CONST_DIR\n",
        "40\t_\t_\tB-LIMIT\n",
        "%\t_\t_\tI-LIMIT\n",
        "of\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "total\t_\t_\tO\n",
        "investment\t_\t_\tO\n",
        "should\t_\t_\tO\n",
        "be\t_\t_\tO\n",
        "allocated\t_\t_\tO\n",
        "to\t_\t_\tO\n",
        "foreign\t_\t_\tB-VAR\n",
        "bonds\t_\t_\tI-VAR\n",
        ".\t_\t_\tO\n",
        "How\t_\t_\tO\n",
        "much\t_\t_\tO\n",
        "should\t_\t_\tO\n",
        "the\t_\t_\tO\n",
        "Cautious\t_\t_\tO\n",
        "Asset\t_\t_\tO\n",
        "Investment\t_\t_\tO\n",
        "allocate\t_\t_\tO\n",
        "in\t_\t_\tO\n",
        "each\t_\t_\tO\n",
        "asset\t_\t_\tO\n",
        "so\t_\t_\tO\n",
        "as\t_\t_\tO\n",
        "to\t_\t_\tO\n",
        "maximize\t_\t_\tB-OBJ_DIR\n",
        "its\t_\t_\tO\n",
        "average\t_\t_\tB-OBJ_NAME\n",
        "return\t_\t_\tI-OBJ_NAME\n",
        "?\t_\t_\tO\n",
        "\n",
        "A\t_\t_\tO\n",
        "flooring\t_\t_\tO\n",
        "company\t_\t_\tO\n",
        "produces\t_\t_\tO\n",
        "engineered\t_\t_\tO\n",
        "hardwood\t_\t_\tB-VAR\n",
        "and\t_\t_\tO\n",
        "vinyl\t_\t_\tB-VAR\n",
        "planks\t_\t_\tI-VAR\n",
        ".\t_\t_\tO\n",
        "Their\t_\t_\tO\n",
        "sales\t_\t_\tO\n",
        "forecasts\t_\t_\tO\n",
        "show\t_\t_\tO\n",
        "an\t_\t_\tO\n",
        "expected\t_\t_\tO\n",
        "demand\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "at\t_\t_\tB-CONST_DIR\n",
        "least\t_\t_\tI-CONST_DIR\n",
        "20,000\t_\t_\tB-LIMIT\n",
        "square\t_\t_\tO\n",
        "foot\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "hardwood\t_\t_\tB-VAR\n",
        "and\t_\t_\tO\n",
        "10,000\t_\t_\tB-LIMIT\n",
        "square\t_\t_\tO\n",
        "feet\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "vinyl\t_\t_\tB-VAR\n",
        "planks\t_\t_\tI-VAR\n",
        "each\t_\t_\tO\n",
        "week\t_\t_\tO\n",
        ".\t_\t_\tO\n",
        "To\t_\t_\tO\n",
        "\n",
        "## Unlabeled text (2):\n",
        "satisfy\n",
        "a\n",
        "shipping\n",
        "contract\n",
        ",\n",
        "a\n",
        "total\n",
        "of\n",
        "at\n",
        "least\n",
        "60,000\n",
        "square\n",
        "feet\n",
        "of\n",
        "flooring\n",
        "much\n",
        "be\n",
        "shipped\n",
        "each\n",
        "week\n",
        ".\n",
        "Due\n",
        "to\n",
        "a\n",
        "labor\n",
        "shortage\n",
        "issue\n",
        ",\n",
        "no\n",
        "more\n",
        "than\n",
        "50,000\n",
        "square\n",
        "feet\n",
        "of\n",
        "hardwood\n",
        "and\n",
        "30,000\n",
        "square\n",
        "feet\n",
        "of\n",
        "vinyl\n",
        "can\n",
        "be\n",
        "produced\n",
        "weekly\n",
        ".\n",
        "If\n",
        "a\n",
        "square\n",
        "foot\n",
        "of\n",
        "hardwood\n",
        "flooring\n",
        "yields\n",
        "a\n",
        "profit\n",
        "of\n",
        "$\n",
        "2.5\n",
        "and\n",
        "a\n",
        "square\n",
        "foot\n",
        "of\n",
        "vinyl\n",
        "planks\n",
        "produces\n",
        "a\n",
        "$\n",
        "3\n",
        "profit\n",
        ",\n",
        "how\n",
        "many\n",
        "of\n",
        "each\n",
        "type\n",
        "of\n",
        "flooring\n",
        "should\n",
        "be\n",
        "made\n",
        "weekly\n",
        "to\n",
        "maximize\n",
        "the\n",
        "company\n",
        "'s\n",
        "profit\n",
        "?\n",
        "\n",
        "John\n",
        "has\n",
        "a\n",
        "300\n",
        "acre\n",
        "berry\n",
        "farm\n",
        "on\n",
        "which\n",
        "to\n",
        "plant\n",
        "blueberries\n",
        "and\n",
        "raspberries\n",
        ".\n",
        "John\n",
        "has\n",
        "$\n",
        "10000\n",
        "to\n",
        "spend\n",
        "on\n",
        "watering\n",
        "and\n",
        "575\n",
        "days\n",
        "worth\n",
        "of\n",
        "labor\n",
        "available\n",
        ".\n",
        "For\n",
        "each\n",
        "acre\n",
        "of\n",
        "blueberries\n",
        ",\n",
        "6\n",
        "days\n",
        "worth\n",
        "of\n",
        "labor\n",
        "and\n",
        "$\n",
        "22\n",
        "in\n",
        "watering\n",
        "costs\n",
        "is\n",
        "required\n",
        ".\n",
        "For\n",
        "each\n",
        "acre\n",
        "of\n",
        "raspberries\n",
        ",\n",
        "3\n",
        "days\n",
        "worth\n",
        "of\n",
        "labor\n",
        "and\n",
        "$\n",
        "25\n",
        "in\n",
        "watering\n",
        "costs\n",
        "is\n",
        "required\n",
        ".\n",
        "The\n",
        "profit\n",
        "per\n",
        "acre\n",
        "of\n",
        "blueberries\n",
        "is\n",
        "$\n",
        "56\n",
        "and\n",
        "the\n",
        "profit\n",
        "per\n",
        "acre\n",
        "of\n",
        "raspberries\n",
        "is\n",
        "$\n",
        "75\n",
        ".\n",
        "Formulate\n",
        "an\n",
        "LP\n",
        "problem\n",
        "in\n",
        "order\n",
        "to\n",
        "maximize\n",
        "profit\n",
        ".\n",
        "\n",
        "An\n",
        "electronics\n",
        "store\n",
        "wants\n",
        "to\n",
        "optimize\n",
        "how\n",
        "many\n",
        "phones\n",
        "and\n",
        "laptops\n",
        "are\n",
        "enough\n",
        "to\n",
        "keep\n",
        "in\n",
        "inventory\n",
        ".\n",
        "A\n",
        "phone\n",
        "will\n",
        "earn\n",
        "the\n",
        "store\n",
        "$\n",
        "120\n",
        "in\n",
        "profits\n",
        ",\n",
        "and\n",
        "a\n",
        "laptop\n",
        "will\n",
        "earn\n",
        "$\n",
        "40\n",
        ".\n",
        "A\n",
        "phone\n",
        "requires\n",
        "1\n",
        "sq\n",
        "ft\n",
        "of\n",
        "floor\n",
        "space\n",
        ",\n",
        "whereas\n",
        "a\n",
        "laptop\n",
        "requires\n",
        "4\n",
        "sq\n",
        "ft\n",
        ".\n",
        "In\n",
        "total\n",
        ",\n",
        "400\n",
        "sq\n",
        "ft\n",
        "of\n",
        "floor\n",
        "## labeled text (2):\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XJ22aRItJDjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output1 = llm(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8gUWx35sHnM",
        "outputId": "4da0bba9-957d-4da4-d67c-909cc3c06f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0bd638890937>:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  output1 = llm(prompt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A\t_\t_\tO\n",
            "flooring\t_\t_\tB-VAR\n",
            "company\t_\t_\tB-ORG\n",
            "produces\t_\t_\tO\n",
            "engineered\t_\t_\tB-ATTR\n",
            "hardwood\t_\t_\tI-VAR\n",
            "and\t_\t_\tO\n",
            "vinyl\t_\t_\tI-VAR\n",
            "planks\t_\t_\tI-ATTR\n",
            ".\t_\t_\tO\n",
            "Their\t_\t_\tO\n",
            "sales\t_\t_\tO\n",
            "forecasts\t_\t_\tO\n",
            "show\t_\t_\tO\n",
            "an\t_\t_\tO\n",
            "expected\t_\t_\tO\n",
            "demand\t_\t_\tB-OBJ_NAME\n",
            "of\t_\t_\tO\n",
            "at\t_\t_\tB-CONST_DIR\n",
            "least\t_\t_\tI-CONST_DIR\n",
            "20,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tB-ATTR\n",
            "foot\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "hardwood\t_\t_\tB-VAR\n",
            "and\t_\t_\tO\n",
            "10,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tO\n",
            "feet\t_\t_\tB-ATTR\n",
            "of\t_\t_\tO\n",
            "vinyl\t_\t_\tB-VAR\n",
            "planks\t_\t_\tI-ATTR\n",
            "each\t_\t_\tO\n",
            "week\t_\t_\tB-OBJ_NAME\n",
            ".\t_\t_\tO\n",
            "To\t_\t_\tO\n",
            "  satisfy\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "shipping\t_\t_\tB-OBJ_DIR\n",
            "contract\t_\t_\tI-OBJ_DIR\n",
            ",\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "total\t_\t_\tB-CONST_DIR\n",
            "of\t_\t_\tO\n",
            "at\t_\t_\tB-CONST_DIR\n",
            "least\t_\t_\tI-CONST_DIR\n",
            "60,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tB-ATTR\n",
            "feet\t_\t_\tB-ATTR\n",
            "of\t_\t_\tO\n",
            "flooring\t_\t_\tB-OBJ_NAME\n",
            "much\t_\t_\tO\n",
            "be\t_\t_\tO\n",
            "shipped\t_\t_\tO\n",
            "each\t_\t_\tO\n",
            "week\t_\t_\tB-OBJ_NAME\n",
            ".\t_\t_\tO\n",
            "Due\t_\t_\tO\n",
            "to\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "labor\t_\t_\tB-OBJ_DIR\n",
            "shortage\t_\t_\tI-OBJ_DIR\n",
            "issue\t_\t_\tO\n",
            ",\t_\t_\tO\n",
            "no\t_\t_\tB-CONST_DIR\n",
            "more\t_\t_\tI-CONST_DIR\n",
            "than\t_\t_\tI-CONST_DIR\n",
            "50,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tB-ATTR\n",
            "feet\t_\t_\tB-ATTR\n",
            "of\t_\t_\tO\n",
            "hardwood\t_\t_\tB-VAR\n",
            "and\t_\t_\tO\n",
            "30,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tO\n",
            "feet\t_\t_\tB-ATTR\n",
            "of\t_\t_\tO\n",
            "vinyl\t_\t_\tB-VAR\n",
            "can\t_\t_\tB-CONST_DIR\n",
            "be\t_\t_\tO\n",
            "produced\t_\t_\tO\n",
            "weekly\t_\t_\tB-OBJ_NAME\n",
            ".\t_\t_\tO\n",
            "If\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "square\t_\t_\tB-ATTR\n",
            "foot\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "hardwood\t_\t_\tB-VAR\n",
            "flooring\t_\t_\tI-VAR\n",
            "yields\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "$\t_\t_\tO\n",
            "2.5\t_\t_\tB-PARAM\n",
            "in\t_\t_\tO\n",
            "profit\t_\t_\tB-OBJ_NAME\n",
            ",\t_\t_\tO\n",
            "and\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "square\t_\t_\tB-ATTR\n",
            "foot\t_\t_\tO\n",
            "of\t_\t_\tO"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   11157.39 ms\n",
            "llama_print_timings:      sample time =     525.16 ms /  1024 runs   (    0.51 ms per token,  1949.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13298.95 ms /  2436 tokens (    5.46 ms per token,   183.17 tokens per second)\n",
            "llama_print_timings:        eval time =   41220.95 ms /  1023 runs   (   40.29 ms per token,    24.82 tokens per second)\n",
            "llama_print_timings:       total time =   60841.37 ms /  3459 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output2 = llm(prompt2)"
      ],
      "metadata": {
        "id": "ayTsBI3qUa2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9826e5bd-658d-43ee-d9bb-4aa7fa287457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "satisfy\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "shipping\t_\t_\tO\n",
            "contract\t_\t_\tO\n",
            ",\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "total\t_\t_\tB-CONST_DIR\n",
            "of\t_\t_\tO\n",
            "at\t_\t_\tB-CONST_DIR\n",
            "least\t_\t_\tI-CONST_DIR\n",
            "60,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tO\n",
            "feet\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "flooring\t_\t_\tO\n",
            "much\t_\t_\tO\n",
            "be\t_\t_\tO\n",
            "shipped\t_\t_\tO\n",
            "each\t_\t_\tO\n",
            "week\t_\t_\tO\n",
            ".\t_\t_\tO\n",
            "Due\t_\t_\tO\n",
            "to\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "labor\t_\t_\tO\n",
            "shortage\t_\t_\tO\n",
            "issue\t_\t_\tO\n",
            ",\t_\t_\tO\n",
            "no\t_\t_\tB-CONST_DIR\n",
            "more\t_\t_\tI-CONST_DIR\n",
            "than\t_\t_\tI-CONST_DIR\n",
            "50,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tO\n",
            "feet\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "hardwood\t_\t_\tB-VAR\n",
            "and\t_\t_\t_\tO\n",
            "30,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tO\n",
            "feet\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "vinyl\t_\t_\tB-VAR\n",
            "can\t_\t_\tO\n",
            "be\t_\t_\tO\n",
            "produced\t_\t_\tO\n",
            "weekly\t_\t_\tO\n",
            ".\t_\t_\tO\n",
            "If\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "square\t_\t_\tO\n",
            "foot\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "hardwood\t_\t_\tB-VAR\n",
            "flooring\t_\t_\tI-VAR\n",
            "yields\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "$\t_\t_\tB-PARAM\n",
            "2.5\t_\t_\tI-PARAM\n",
            "profit\t_\t_\tB-OBJ_NAME\n",
            ",\t_\t_\tO\n",
            "and\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "square\t_\t_\tO\n",
            "foot\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "vinyl\t_\t_\tB-VAR\n",
            "planks\t_\t_\tI-VAR\n",
            "produces\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "$\t_\t_\tB-PARAM\n",
            "3\t_\t_\tI-PARAM\n",
            "profit\t_\t_\tB-OBJ_NAME\n",
            ",\t_\t_\tO\n",
            "how\t_\t_\tO\n",
            "many\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "each\t_\t_\tO\n",
            "type\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "flooring\t_\t_\tO\n",
            "should\t_\t_\tO\n",
            "be\t_\t\tO\n",
            "made\t_\t_\tO\n",
            "weekly\t_\t_\tO\n",
            "to\t_\t_\tO\n",
            "maximize\t_\t_\tB-OBJ_DIR\n",
            "the\t_\t_\tO\n",
            "company\t_\t_\tO\n",
            "'s\t_\t_\tO\n",
            "profit\t_\t_\tI-OBJ_NAME\n",
            "?\t_\t_\tO\n",
            "\n",
            "John\t_\t_\tO\n",
            "has\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "300\t_\t_\tB-LIMIT\n",
            "acre\t_\t_\tI-LIMIT\n",
            "berry\t_\t_\tB-VAR\n",
            "farm\t_\t_\tI-VAR\n",
            "on\t_\t_\tO\n",
            "which\t_\t_\tO\n",
            "to\t_\t_\tO\n",
            "plant\t_\t_\tO\n",
            "blueberries\t_\t_\tB-VAR\n",
            "and\t_\t_\t_\tO\n",
            "raspberries\t_\t_\tB-VAR\n",
            ".\t_\t_\tO\n",
            "John\t_\t_\tO\n",
            "has\t_\t_\tO\n",
            "$\t_\t_\tB-PARAM\n",
            "10,000\t_\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =   11157.39 ms\n",
            "llama_print_timings:      sample time =     511.46 ms /  1024 runs   (    0.50 ms per token,  2002.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2847.01 ms /  2492 tokens (    1.14 ms per token,   875.31 tokens per second)\n",
            "llama_print_timings:        eval time =   42699.24 ms /  1023 runs   (   41.74 ms per token,    23.96 tokens per second)\n",
            "llama_print_timings:       total time =   51618.97 ms /  3515 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "x55-EqQNNE4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First we need to check if all the provided text got labeled and remove any excess"
      ],
      "metadata": {
        "id": "D7_Rc5MOpt1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_first_word(text):\n",
        "    # Split the text by lines\n",
        "    lines = text.splitlines()\n",
        "\n",
        "    # Extract the first word from each line\n",
        "    first_words = [line.split()[0] for line in lines if line.strip()]\n",
        "\n",
        "    # Join the first words into a single string with newlines\n",
        "    result = \"\\n\".join(first_words)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example input\n",
        "text = \"\"\"satisfy\t_\t_\tO\n",
        "a\t_\t_\tO\n",
        "shipping\t_\t_\tO\n",
        "contract\t_\t_\tO\n",
        ",\t_\t_\tO\n",
        "a\t_\t_\tO\n",
        "total\t_\t_\tB-CONST_DIR\n",
        "of\t_\t_\tO\n",
        "at\t_\t_\tB-CONST_DIR\n",
        "least\t_\t_\tI-CONST_DIR\n",
        "60,000\t_\t_\tB-LIMIT\n",
        "square\t_\t_\tO\n",
        "feet\t_\t_\tO\n",
        "of\t_\t_\tO\n",
        "flooring\t_\t_\tO\n",
        "much\t_\t_\tO\n",
        "be\t_\t_\tO\n",
        "shipped\t_\t_\tO\"\"\"\n",
        "\n",
        "# Extract and print the result\n",
        "result = extract_first_word(text)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylM-30_sz6pS",
        "outputId": "b7a071e5-3352-4297-bdb5-54e7e500578e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "satisfy\n",
            "a\n",
            "shipping\n",
            "contract\n",
            ",\n",
            "a\n",
            "total\n",
            "of\n",
            "at\n",
            "least\n",
            "60,000\n",
            "square\n",
            "feet\n",
            "of\n",
            "flooring\n",
            "much\n",
            "be\n",
            "shipped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_texts(text1, text2):\n",
        "    # Split the texts by line and treat each line as a word\n",
        "    words1 = [word.strip() for word in text1.splitlines() if word.strip()]\n",
        "    words2 = [word.strip() for word in text2.splitlines() if word.strip()]\n",
        "\n",
        "    # Convert lists to sets to compare\n",
        "    set1 = set(words1)\n",
        "    set2 = set(words2)\n",
        "\n",
        "    # Find common words\n",
        "    common_words = set1.intersection(set2)\n",
        "\n",
        "    # Calculate the percentage of common words\n",
        "    total_unique_words = len(set1.union(set2))\n",
        "    common_percentage = (len(common_words) / total_unique_words) * 100\n",
        "\n",
        "    # Find words in text1 but not in text2\n",
        "    words_only_in_text1 = set1 - set2\n",
        "\n",
        "    # Find words in text2 but not in text1\n",
        "    words_only_in_text2 = set2 - set1\n",
        "\n",
        "    # Output results\n",
        "    print(f\"Percentage of common words: {common_percentage:.2f}%\")\n",
        "    print(f\"Words only in the first text: {list(words_only_in_text1)}\")\n",
        "    print(f\"Words only in the second text: {list(words_only_in_text2)}\")\n",
        "\n",
        "    return common_percentage, list(words_only_in_text1), list(words_only_in_text2)\n",
        "\n",
        "\n",
        "common_percentage, missing_in_text1, missing_in_text2 = compare_texts(input_text, extract_first_word(output1))\n",
        "common_percentage, missing_in_text1, missing_in_text2 = compare_texts(input_text, extract_first_word(output2))\n"
      ],
      "metadata": {
        "id": "UutSzrXJTO_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f4d859-bd96-45c6-eb1b-d59735dc8d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of common words: 36.72%\n",
            "Words only in the first text: ['on', '6', 'ft', '25', 'how', 'profits', '4', '400', 'enough', '75', 'laptops', 'spend', 'floor', 'space', 'electronics', 'inventory', 'requires', 'costs', 'wants', 'John', 'available', 'many', '?', 'which', 'whereas', 'For', 'berry', 'should', 'earn', 'blueberries', '10000', 'An', 'store', '40', '575', '3', \"'s\", 'has', 'farm', 'LP', 'Formulate', 'required', 'made', 'sq', 'the', 'problem', 'per', 'optimize', 'phone', 'laptop', 'type', 'is', 'The', 'keep', 'In', 'worth', 'plant', 'maximize', 'watering', 'are', '22', 'raspberries', 'phones', '120', 'acre', 'days', '1', 'order', '300', '56', 'will']\n",
            "Words only in the second text: ['forecasts', 'Their', 'engineered', 'demand', 'expected', 'To', '20,000', 'sales', 'show', '10,000']\n",
            "Percentage of common words: 54.62%\n",
            "Words only in the first text: ['6', 'ft', '25', 'profits', 'an', '4', 'A', '400', 'enough', '75', 'laptops', 'spend', 'floor', 'space', 'electronics', 'inventory', 'requires', 'costs', 'wants', 'available', 'For', 'whereas', 'earn', '10000', 'An', 'store', '40', '575', 'LP', 'in', 'Formulate', 'required', 'sq', 'problem', 'optimize', 'phone', 'laptop', 'is', 'keep', 'The', 'In', 'worth', 'watering', 'are', '22', 'phones', '120', 'days', '1', 'order', 'per', '56', 'will']\n",
            "Words only in the second text: ['10,000']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output1)"
      ],
      "metadata": {
        "id": "6owoyFLH1Sht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5ad3ab-9974-4b18-8db8-b8fd41057e76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A\t_\t_\tO\n",
            "flooring\t_\t_\tB-VAR\n",
            "company\t_\t_\tB-ORG\n",
            "produces\t_\t_\tO\n",
            "engineered\t_\t_\tB-ATTR\n",
            "hardwood\t_\t_\tI-VAR\n",
            "and\t_\t_\tO\n",
            "vinyl\t_\t_\tI-VAR\n",
            "planks\t_\t_\tI-ATTR\n",
            ".\t_\t_\tO\n",
            "Their\t_\t_\tO\n",
            "sales\t_\t_\tO\n",
            "forecasts\t_\t_\tO\n",
            "show\t_\t_\tO\n",
            "an\t_\t_\tO\n",
            "expected\t_\t_\tO\n",
            "demand\t_\t_\tB-OBJ_NAME\n",
            "of\t_\t_\tO\n",
            "at\t_\t_\tB-CONST_DIR\n",
            "least\t_\t_\tI-CONST_DIR\n",
            "20,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tB-ATTR\n",
            "foot\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "hardwood\t_\t_\tB-VAR\n",
            "and\t_\t_\tO\n",
            "10,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tO\n",
            "feet\t_\t_\tB-ATTR\n",
            "of\t_\t_\tO\n",
            "vinyl\t_\t_\tB-VAR\n",
            "planks\t_\t_\tI-ATTR\n",
            "each\t_\t_\tO\n",
            "week\t_\t_\tB-OBJ_NAME\n",
            ".\t_\t_\tO\n",
            "To\t_\t_\tO\n",
            "  satisfy\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "shipping\t_\t_\tB-OBJ_DIR\n",
            "contract\t_\t_\tI-OBJ_DIR\n",
            ",\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "total\t_\t_\tB-CONST_DIR\n",
            "of\t_\t_\tO\n",
            "at\t_\t_\tB-CONST_DIR\n",
            "least\t_\t_\tI-CONST_DIR\n",
            "60,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tB-ATTR\n",
            "feet\t_\t_\tB-ATTR\n",
            "of\t_\t_\tO\n",
            "flooring\t_\t_\tB-OBJ_NAME\n",
            "much\t_\t_\tO\n",
            "be\t_\t_\tO\n",
            "shipped\t_\t_\tO\n",
            "each\t_\t_\tO\n",
            "week\t_\t_\tB-OBJ_NAME\n",
            ".\t_\t_\tO\n",
            "Due\t_\t_\tO\n",
            "to\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "labor\t_\t_\tB-OBJ_DIR\n",
            "shortage\t_\t_\tI-OBJ_DIR\n",
            "issue\t_\t_\tO\n",
            ",\t_\t_\tO\n",
            "no\t_\t_\tB-CONST_DIR\n",
            "more\t_\t_\tI-CONST_DIR\n",
            "than\t_\t_\tI-CONST_DIR\n",
            "50,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tB-ATTR\n",
            "feet\t_\t_\tB-ATTR\n",
            "of\t_\t_\tO\n",
            "hardwood\t_\t_\tB-VAR\n",
            "and\t_\t_\tO\n",
            "30,000\t_\t_\tB-LIMIT\n",
            "square\t_\t_\tO\n",
            "feet\t_\t_\tB-ATTR\n",
            "of\t_\t_\tO\n",
            "vinyl\t_\t_\tB-VAR\n",
            "can\t_\t_\tB-CONST_DIR\n",
            "be\t_\t_\tO\n",
            "produced\t_\t_\tO\n",
            "weekly\t_\t_\tB-OBJ_NAME\n",
            ".\t_\t_\tO\n",
            "If\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "square\t_\t_\tB-ATTR\n",
            "foot\t_\t_\tO\n",
            "of\t_\t_\tO\n",
            "hardwood\t_\t_\tB-VAR\n",
            "flooring\t_\t_\tI-VAR\n",
            "yields\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "$\t_\t_\tO\n",
            "2.5\t_\t_\tB-PARAM\n",
            "in\t_\t_\tO\n",
            "profit\t_\t_\tB-OBJ_NAME\n",
            ",\t_\t_\tO\n",
            "and\t_\t_\tO\n",
            "a\t_\t_\tO\n",
            "square\t_\t_\tB-ATTR\n",
            "foot\t_\t_\tO\n",
            "of\t_\t_\tO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSRPNtXFemLp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
